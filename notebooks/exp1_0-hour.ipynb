{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasts\n",
    "\n",
    "We want to test for a linear relationship between the 0-hour predictions for longwave and shortwave flux (`DLWRF_SFC` and `DSWRF_SFC`) and the target data collected at the solar farm. The solar farm is located on the map at position (91, 81) in the (y, x) dimensions.\n",
    "\n",
    "We can use the `uga_solar.data.nam` package to load the forecasts. Loading forecasts can take a while, so you should only load a small subset while working. The function `nam.open_range(start, end)` allows us to open open a subset of the data by timestamp.\n",
    "\n",
    "The data is returned as an xarray `Dataset`. We can use the xarray API to select specific variables and slice any dimension. The `.sel` method allows us to select along dimensions by their coordinate labels, and `.isel` allows us to select by position index. We use these to slice the y, x, and forecast dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uga_solar.data import nam\n",
    "inputs = nam.open_range('2016-12-01T00:00', '2017-11-01T00:00')\n",
    "inputs = inputs[['DSWRF_SFC', 'DLWRF_SFC']]\n",
    "inputs = inputs.isel(y=slice(90, 93))\n",
    "inputs = inputs.isel(x=slice(80, 83))\n",
    "inputs = inputs.isel(forecast=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "The targets can be loaded with the `uga_solar.data.ga_power` module. The `open_aggregate` function opens all target logs of the given module and computes their average over a 1-hour time interval. The result is cached to disk. The value returned is a pandas `Dataframe`.\n",
    "\n",
    "Module 7 has most of the important stuff. We select column 5 as the target, which is the ventilated pyranometer on the 2-axis array. We also drop rows with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uga_solar.data import ga_power\n",
    "targets = ga_power.open_aggregate(7)\n",
    "targets = targets[5]\n",
    "targets = targets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader\n",
    "\n",
    "Now that we have our data, we need to adapt it to the PyTorch `DataSet` interface which allows it to be iterated over in batches with a PyTorch `DataLoader`. The `uga_solar.data` package has some tools to help us out.\n",
    "\n",
    "The `XarrayDataset` class (name subject to change) adapts our data to the PyTorch interface. It takes an xarray Dataset as its first argument followed by any number of pandas DataFrames. The DataFrames are joined against the `reftime` dimension of the xarray dataset.\n",
    "\n",
    "The `CrossValLoader` class is a factory for PyTorch `DataLoader`s with cross-validation support. For each fold, it generates 3 dataloaders for the train set, test set, and validation set. If we have $n$ samples divided into $k$ folds, then the test and validation sets each contain $n/k$ samples while the train set contains the remaining $n(k-2)/k$ samples.\n",
    "\n",
    "We create a loader as an example and use it to load a single batch. We can use the inputs and targets returned from this batch to help define the shape of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([32, 2, 1, 3, 3]) torch.DoubleTensor\n",
      "targets: torch.Size([32]) torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "from uga_solar.data import XarrayDataset, CrossValLoader\n",
    "dataset = XarrayDataset(inputs, targets)\n",
    "cv = CrossValLoader(dataset)\n",
    "epoch = iter(cv.load_val(0))\n",
    "batch = next(epoch)\n",
    "x, y = batch\n",
    "\n",
    "print('inputs:', x.shape, x.type())\n",
    "print('targets:', y.shape, y.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network\n",
    "\n",
    "The `Net` class defines our network. We're just using a simple linear regression for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.in_size = int(np.prod(in_shape))\n",
    "        self.out_size = int(np.prod(out_shape))\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.linear = nn.Linear(self.in_size, self.out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_size)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, *self.out_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Estimator` class wraps a network, cost function, and optimizer into a scikit-learn-like interface.\n",
    "\n",
    "The network and cost functions are both instances of [`torch.nn.Module`](http://pytorch.org/docs/master/nn.html) and the optimizer is an instance of [`torch.optim.Optimizer`](http://pytorch.org/docs/master/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self, net, cost, optimizer, dtype=None):\n",
    "        if dtype is None:\n",
    "            if torch.cuda.is_available():\n",
    "                dtype = torch.cuda.FloatTensor\n",
    "            else:\n",
    "                dtype = torch.FloatTensor\n",
    "        self.dtype = dtype\n",
    "        self.net = net.type(self.dtype)\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def partial_fit(self, x, y):\n",
    "        self.net.train(True)\n",
    "        x = autograd.Variable(x).type(self.dtype)\n",
    "        y = autograd.Variable(y).type(self.dtype)\n",
    "        self.optimizer.zero_grad()\n",
    "        h = self.net(x)\n",
    "        j = self.cost(h, y)\n",
    "        j.backward()\n",
    "        self.optimizer.step()\n",
    "        return j\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.net.train(False)\n",
    "        x = autograd.Variable(x).type(self.dtype)\n",
    "        h = self.net(x)\n",
    "        return h\n",
    "    \n",
    "    def score(self, x, y, criteria=None):\n",
    "        self.net.train(False)\n",
    "        criteria = criteria or self.cost\n",
    "        x = autograd.Variable(x).type(self.dtype)\n",
    "        y = autograd.Variable(y).type(self.dtype)\n",
    "        h = self.net(x)\n",
    "        j = criteria(h, y)\n",
    "        return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an experiment\n",
    "\n",
    "Now we have everything we need to run an experiment.\n",
    "\n",
    "For each fold, we create a new estimator. We train it for some number of epochs, and at the end of each epoch, we evaluate it against the validation set. Finally, we evaluate each model on their respective test sets.\n",
    "\n",
    "Currently we're only testing the final model from each fold. A more complete experiment would instead save the model which had the best validation loss across all epochs, and test that model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> fold 0\n",
      "epoch 01 .....................[Train loss: 1.03e+07][Val loss: 1.15e+04]\n",
      "epoch 02 .....................[Train loss: 6.29e+06][Val loss: 4.88e+04]\n",
      "epoch 03 .....................[Train loss: 4.43e+06][Val loss: 2.04e+05]\n",
      "epoch 04 .....................[Train loss: 3.79e+06][Val loss: 3.54e+05]\n",
      "epoch 05 .....................[Train loss: 3.64e+06][Val loss: 4.56e+05]\n",
      "epoch 06 .....................[Train loss: 3.64e+06][Val loss: 5.14e+05]\n",
      "epoch 07 .....................[Train loss: 3.65e+06][Val loss: 5.43e+05]\n",
      "epoch 08 .....................[Train loss: 3.64e+06][Val loss: 5.56e+05]\n",
      "epoch 09 .....................[Train loss: 3.63e+06][Val loss: 5.62e+05]\n",
      "epoch 10 .....................[Train loss: 3.61e+06][Val loss: 5.63e+05]\n",
      "epoch 11 .........[Val loss: 5.64e+05]\n",
      "epoch 13 .....................[Train loss: 3.53e+06][Val loss: 5.65e+05]\n",
      "epoch 14 .....................[Train loss: 3.51e+06][Val loss: 5.66e+05]\n",
      "epoch 15 .....................[Train loss: 3.48e+06][Val loss: 5.68e+05]\n",
      "epoch 16 .....................[Train loss: 3.46e+06][Val loss: 5.70e+05]\n",
      "epoch 17 .....................[Train loss: 3.43e+06][Val loss: 5.74e+05]\n",
      "epoch 18 .....................[Train loss: 3.41e+06][Val loss: 5.77e+05]\n",
      "epoch 19 .....................[Train loss: 3.38e+06][Val loss: 5.82e+05]\n",
      "epoch 20 .....................[Train loss: 3.36e+06][Val loss: 5.87e+05]\n",
      "epoch 21 .....................[Train loss: 3.34e+06][Val loss: 5.93e+05]\n",
      "epoch 22 .....................[Train loss: 3.32e+06][Val loss: 5.99e+05]\n",
      "epoch 23 .....................[Train loss: 3.29e+06][Val loss: 6.06e+05]\n",
      "epoch 24 .....................[Train loss: 3.27e+06][Val loss: 6.14e+05]\n",
      "epoch 25 .....................[Train loss: 3.25e+06][Val loss: 6.23e+05]\n",
      "epoch 26 .....................[Train loss: 3.23e+06][Val loss: 6.32e+05]\n",
      "epoch 27 .....................[Train loss: 3.21e+06][Val loss: 6.42e+05]\n",
      "epoch 28 .....................[Train loss: 3.19e+06][Val loss: 6.52e+05]\n",
      "epoch 29 .....................[Train loss: 3.17e+06][Val loss: 6.64e+05]\n",
      "epoch 30 .....................[Train loss: 3.15e+06][Val loss: 6.76e+05]\n",
      "epoch 31 .....................[Train loss: 3.14e+06][Val loss: 6.88e+05]\n",
      "epoch 32 .....................[Train loss: 3.12e+06][Val loss: 7.02e+05]\n",
      "epoch 33 .....................[Train loss: 3.10e+06][Val loss: 7.16e+05]\n",
      "epoch 34 .....................[Train loss: 3.08e+06][Val loss: 7.30e+05]\n",
      "epoch 35 .....................[Train loss: 3.07e+06][Val loss: 7.46e+05]\n",
      "epoch 36 .....................[Train loss: 3.05e+06][Val loss: 7.62e+05]\n",
      "epoch 37 .....................[Train loss: 3.03e+06][Val loss: 7.78e+05]\n",
      "epoch 38 .....................[Train loss: 3.01e+06][Val loss: 7.96e+05]\n",
      "epoch 39 .....................[Train loss: 3.00e+06][Val loss: 8.14e+05]\n",
      "epoch 40 .....................[Train loss: 2.98e+06][Val loss: 8.33e+05]\n",
      "epoch 41 .....................[Train loss: 2.97e+06][Val loss: 8.53e+05]\n",
      "epoch 42 .....................[Train loss: 2.95e+06][Val loss: 8.73e+05]\n",
      "epoch 43 .....................[Train loss: 2.94e+06][Val loss: 8.94e+05]\n",
      "epoch 44 ..................."
     ]
    }
   ],
   "source": [
    "cv = CrossValLoader(dataset, n_folds=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print('>>> fold', i)\n",
    "    net = Net(x.shape[1:], y.shape[1:])\n",
    "    cost = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    estimator = Estimator(net, cost, optimizer)\n",
    "    train_set = cv.load_train(i)\n",
    "    val_set = cv.load_val(i)\n",
    "    test_set = cv.load_test(i)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        print(f'epoch {epoch+1:02d} ', end='')\n",
    "        \n",
    "        # train\n",
    "        loss = 0\n",
    "        n = len(train_set)\n",
    "        for x, y in train_set:\n",
    "            j = estimator.partial_fit(x, y)\n",
    "            loss += j * (len(x) / n)\n",
    "            print(end='.', flush=True)\n",
    "        loss = loss.data.cpu().numpy().squeeze()\n",
    "        print(f'[Train loss: {float(loss):8.2e}]', end='')\n",
    "            \n",
    "        # evaluate - val set\n",
    "        loss = 0\n",
    "        n = len(val_set)\n",
    "        for x, y in val_set:\n",
    "            j = estimator.score(x, y)\n",
    "            loss += j * (len(x) / n)\n",
    "        loss = loss.data.cpu().numpy().squeeze()\n",
    "        print(f'[Val loss: {float(loss):8.2e}]')\n",
    "        \n",
    "    # evaluate - test set\n",
    "    loss = 0\n",
    "    n = len(test_set)\n",
    "    for x, y in test_set:\n",
    "        j = estimator.score(x, y)\n",
    "        loss += j * (len(x) / n)\n",
    "    loss = loss.data.cpu().numpy().squeeze()\n",
    "    print(f'[Test loss: {float(loss):8.2e}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csb]",
   "language": "python",
   "name": "conda-env-csb-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
